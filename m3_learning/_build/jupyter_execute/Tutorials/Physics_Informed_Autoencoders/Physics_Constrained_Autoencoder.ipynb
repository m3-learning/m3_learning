{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxrwNCW3d_W5",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Physics Constrained Autoencoders\n",
    "\n",
    "By Mary Ye$^1$, Joshua C. Agar$^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxrwNCW3d_W5",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$^1$ Department of Computer science and Engineering, Lehigh University\n",
    "$^2$ Department of Mechanical Engineering and Mechanics, Drexel University\n",
    "\n",
    "- There are many times where you want to fit spectroscopic data to a model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Classical fitting methods can be used but break down:\n",
    "  - When data is noisy\n",
    "  - There are multiple candidate models\n",
    "  - Data is high velocity\n",
    "  - Data is noisy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oWjlBBJzs73",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Imports Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install m3_learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g0VKiyka604-",
    "outputId": "272c5a5b-36ac-4db0-b3d9-7b88401a9e13",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import math\n",
    "\n",
    "from m3_learning.nn.time_series_nn.nn_util import Train, transform_nn\n",
    "from m3_learning.viz.layout import layout_fig\n",
    "from m3_learning.viz.nn import embeddings, latent_generator\n",
    "from m3_learning.util.rand_util import rand_tensor\n",
    "from m3_learning.viz.style import set_style\n",
    "from m3_learning.nn.random import random_seed\n",
    "from m3_learning.viz.nn import embeddings, latent_generator\n",
    "set_style(\"printing\")\n",
    "random_seed(seed=42)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8x-NFM-D0hpm",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generating some data based on the image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8x-NFM-D0hpm",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Define a non-linear function\n",
    "\n",
    "<center> $$ y = A sin(2\\theta f+ \\phi)$$ </center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o1XGgjpuIwTx",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Sin_func:\n",
    "\n",
    "    \"\"\"\n",
    "    Class that computes the Sin function\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        x_vector,\n",
    "        amp=[0, 1],\n",
    "        phase=[0, 1],\n",
    "        frequency=[0, 1],\n",
    "        size=(1, 1),\n",
    "        batch_size=1000,\n",
    "        verbose=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            x_vector:\n",
    "            sd (array, float): range for the standard deviation\n",
    "            mean (array, float): range for the mean\n",
    "            amp (array, float): range for the amplitude\n",
    "            size (tuple): Size of the array first index is number of channels, second is number of functions\n",
    "            verbose (bool): shows outputs\n",
    "        \"\"\"\n",
    "\n",
    "        self.x_vector = x_vector\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.amp = amp\n",
    "        self.amp_mean = torch.tensor(amp[0] + amp[1]) / 2\n",
    "        self.amp_sd = torch.sqrt(\n",
    "            torch.pow(torch.tensor(amp[1]) - torch.tensor(amp[0]), 2) / 12\n",
    "        )\n",
    "\n",
    "        self.phase = phase\n",
    "        self.phase_mean = torch.tensor(phase[0] + phase[1]) / 2\n",
    "        self.phase_sd = torch.sqrt(\n",
    "            torch.pow(torch.tensor(phase[1]) - torch.tensor(phase[0]), 2) / 12\n",
    "        )\n",
    "\n",
    "        self.frequency = frequency\n",
    "        self.frequency_mean = torch.tensor(frequency[0] + frequency[1]) / 2\n",
    "        self.frequency_sd = torch.sqrt(\n",
    "            torch.pow(torch.tensor(frequency[1]) -\n",
    "                      torch.tensor(frequency[0]), 2) / 12\n",
    "        )\n",
    "\n",
    "        self.size = size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def compute(self, params, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            self (object): Returns the instance itself.\n",
    "            device (string, optional) : Sets the device to do the computation. Default `cpu`, common option `cuda`\n",
    "\n",
    "        Returns: out (Tensor): spectra.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if len(params.size()) == 2:\n",
    "            params = torch.reshape(params, (params.shape[0], 3, -1))\n",
    "\n",
    "        out = torch.zeros(\n",
    "            (params.shape[0], self.x_vector.shape[0],\n",
    "             self.size[0], self.size[1])\n",
    "        )\n",
    "\n",
    "        params = params.to(device)\n",
    "\n",
    "        for i in range(self.size[1]):\n",
    "\n",
    "            if params.ndim == 4:\n",
    "                _amp = params[:, 0, 0, i]\n",
    "                _phase = params[:, 0, 1, i]\n",
    "                _frequency = params[:, 0, 2, i]\n",
    "\n",
    "            if params.ndim == 3:\n",
    "                _amp = params[:, 0, i]\n",
    "                _phase = params[:, 1, i]\n",
    "                _frequency = params[:, 2, i]\n",
    "\n",
    "            x_vector = (\n",
    "                torch.cat(params.shape[0] * [self.x_vector])\n",
    "                .reshape(params.shape[0], -1)\n",
    "                .to(device)\n",
    "            )\n",
    "            x_vector = torch.transpose(x_vector, 0, 1)  # .to(device)\n",
    "\n",
    "            _out = _amp * torch.sin(\n",
    "                2 * torch.tensor(np.pi) * _frequency * x_vector + _phase\n",
    "            )\n",
    "\n",
    "            out[:, :, 0, i] = torch.transpose(_out, 0, 1)\n",
    "\n",
    "        return (torch.sum(out, dim=3), out)\n",
    "\n",
    "    def sampler(self, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            device (str): device where computation happens\n",
    "\n",
    "        Returns:\n",
    "            out (Tensor) : Generated spectra\n",
    "            params (Tensor) : parameters used for generation\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        phase = rand_tensor(\n",
    "            min=self.phase[0],\n",
    "            max=self.phase[1],\n",
    "            size=(self.batch_size, self.size[0], self.size[1]),\n",
    "        )\n",
    "        frequency = rand_tensor(\n",
    "            min=self.frequency[0],\n",
    "            max=self.frequency[1],\n",
    "            size=(self.batch_size, self.size[0], self.size[1]),\n",
    "        )\n",
    "        amp = rand_tensor(\n",
    "            min=self.amp[0],\n",
    "            max=self.amp[1],\n",
    "            size=(self.batch_size, self.size[0], self.size[1]),\n",
    "        )\n",
    "        _params = torch.torch.stack((amp, phase, frequency))\n",
    "\n",
    "        _params = torch.atleast_2d(_params)\n",
    "        _params = torch.transpose(_params, 0, 1)\n",
    "        _params = torch.transpose(_params, 1, 2)\n",
    "\n",
    "        return (self.compute(_params, device=device), _params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ziSPN95kMH96",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "constructor = Sin_func(\n",
    "    amp=[0.2, 1],  # Sets the amplitude\n",
    "    phase=[0, 2 * np.pi],  # Sets the phase\n",
    "    frequency=[0.1, 0.5],  # Sets the frequency\n",
    "    x_vector=torch.linspace(0, np.pi, 100),  # Sets the x_vector\n",
    "    batch_size=10000,\n",
    ")  # number of samples to generate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oLaP-ksVM-Vv",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# initializes the constructor\n",
    "output = constructor.sampler()\n",
    "\n",
    "# grabs the parameters and the spectra\n",
    "spectra, params = output\n",
    "\n",
    "# This grabs the sum of all spectral and the individual spectra if they exist\n",
    "spectra_full, spectras = spectra\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fpr1aijpYuO",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Visualize examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "h7CLS6s0NLT4",
    "outputId": "88ed9e63-6940-41b3-cabf-1cdfce3a693c"
   },
   "outputs": [],
   "source": [
    "rand = np.random.randint(0, 10000)\n",
    "plt.plot(spectras[rand, :, 0].cpu(), \"b\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HASHdT3YPCe3",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recurrent Neural Network Autoencoders\n",
    "\n",
    "- It is important to consider the temporal domain\n",
    "- This can be improved by using a recurrent neural network that processes each time step sequentially.\n",
    "- To add an understanding about the short and long term information in the data you can add memory and forget logic as a learnable parameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HASHdT3YPCe3",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://github.com/jagar2/AI_For_Atoms_Autoencoder_Tutorial/blob/main/img/Autoencoder_Med.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMyui0C1PCe3",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://github.com/jagar2/AI_For_Atoms_Autoencoder_Tutorial/blob/main/img/LSTM%20Node.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfc1_DKg89Il",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Builds the autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q6sCf2kDPCe3",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim=12):\n",
    "        self.latent_dim = latent_dim\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(1, 12, batch_first=True, bidirectional=True)\n",
    "        self.lstm2 = nn.LSTM(24, 12, batch_first=True, bidirectional=True)\n",
    "        self.embedding = nn.Linear(24, self.latent_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        (x, (_, __)) = self.lstm(x)\n",
    "        (x, (_, __)) = self.lstm2(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.embedding(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=12):\n",
    "        self.latent_dim = latent_dim\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(self.latent_dim, 12,\n",
    "                            batch_first=True, bidirectional=True)\n",
    "        self.lstm2 = nn.LSTM(24, 12, batch_first=True, bidirectional=True)\n",
    "        self.tdd = nn.Conv1d(24, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x[:, :, None]\n",
    "        x = x.transpose(1, 2)\n",
    "        x = x.repeat([1, 100, 1])\n",
    "        (x, (_, __)) = self.lstm(x)\n",
    "        (x, (_, __)) = self.lstm2(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.tdd(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OxBAeRAjn3j3",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # encode\n",
    "\n",
    "        embedding = self.encoder(x)\n",
    "\n",
    "        # decode\n",
    "\n",
    "        predicted = self.decoder(embedding)\n",
    "\n",
    "        return predicted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NaRlMbfttiaZ",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since we know there are intrinsically 3 latent dimensions let's try and train the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mm-1aG9rPCe4",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "latent_dim = 3\n",
    "\n",
    "encoder = Encoder(latent_dim=latent_dim).to(device)\n",
    "decoder = Decoder(latent_dim=latent_dim).to(device)\n",
    "model = Autoencoder(encoder, decoder).to(device)\n",
    "\n",
    "# optimizer\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q5t5hvfQPCe4",
    "outputId": "e17abdb6-4993-416c-c751-1e1b48cdcab0",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# views the model\n",
    "\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Builds the dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0jldeW4OZSb",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# constructs a dataloader for training\n",
    "\n",
    "dataloader = DataLoader(spectra_full, batch_size=512,\n",
    "                        shuffle=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eijf5211Qa7h",
    "outputId": "fb0e95a7-da1f-455c-d670-aa7c02c622ac",
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# trains the model\n",
    "\n",
    "random_seed(seed=42)\n",
    "\n",
    "Train(\n",
    "    model, encoder, decoder, dataloader, optimizer, 500,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfcYvUY4wjMz",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Visualize the reconstruction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B8fpD4ncv29H",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# computes an example reconstruction for a mini batch\n",
    "\n",
    "(encoded_, decoded_) = transform_nn(next(iter(dataloader)), encoder, decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "NA20y3ijVcwc",
    "outputId": "8aa05dc5-c8cd-4a09-b119-70f58fe1e0c2",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# plots a random example of the original and predicted spectra\n",
    "\n",
    "rand = np.random.randint(0, 512)\n",
    "plt.plot(spectras[rand, :, 0].cpu(), \"b\", label=\"Original\")\n",
    "plt.plot(decoded_[rand].squeeze(), \"r\", label=\"Generated\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXjwNNaSz5MS",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generating Validation Data\n",
    "\n",
    "- We want to generate a hyperspectral image\n",
    "- This can be done by taking the RGB values of an image and using them as parameters for a function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJLKait50MP6",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Loads and image of my dog Nala\n",
    "\n",
    "- Painting by _Irene Dogmatic_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MG4YZi2tfeSi",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Loads dog image\n",
    "\n",
    "image = io.imread(\n",
    "    \"https://github.com/jagar2/m3_learning/blob/main/m3_learning/Tutorials/Unsupervised_Learning_with_AEs/figs/nala.jpg?raw=true\"\n",
    ")\n",
    "\n",
    "# Crops dog image\n",
    "\n",
    "image = image[200:1900:20, 100:1500:20] / 255\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUpSmdfP0Ysv",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Displays the image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "ud8Peh1Sd9CJ",
    "outputId": "b0a04e29-83b9-4bc7-d276-ab523bde36eb",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Generates the data from RGB sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pBKSfOPExI3q",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Converts the image into parameters within the generated range\n",
    "\n",
    "nala_params = np.atleast_3d(image.reshape(-1, 3))\n",
    "\n",
    "nala_amp = torch.tensor(nala_params[:, 0, 0] * 0.8 + 0.2)\n",
    "nala_phase = torch.tensor(nala_params[:, 1, 0] * 2 * np.pi)\n",
    "nala_frequency = torch.tensor(nala_params[:, 2, 0] * 0.5 + 0.1)\n",
    "\n",
    "_nala_params = torch.torch.stack((nala_amp, nala_phase, nala_frequency))\n",
    "\n",
    "_nala_params = torch.atleast_3d(_nala_params)\n",
    "_nala_params = torch.transpose(_nala_params, 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_zD6eocyTO8",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# builds the spectra from the parameters\n",
    "\n",
    "(nala_spectra, _) = constructor.compute(_nala_params)\n",
    "\n",
    "# generated the encoded representation and decoded spectra\n",
    "\n",
    "(nala_encoded_, nala_decoded_) = transform_nn(nala_spectra, encoder, decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "QIIx6V00yW2A",
    "outputId": "2e3117ae-5829-403a-90b3-72a2b3cd8f08",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# plots a random example of the original and predicted spectra\n",
    "\n",
    "rand = np.random.randint(0, nala_spectra.shape[0])\n",
    "plt.plot(nala_spectra[rand, :, 0].cpu(), \"b\", label=\"Original\")\n",
    "plt.plot(nala_decoded_[rand].squeeze(), \"r\", label=\"Generated\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Visualize the learned results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "id": "pTGCN2t0yfc5",
    "outputId": "cd8014ca-117d-4744-95d8-becce3f56df5",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the learned embeddings\n",
    "embeddings(nala_encoded_, shape_=image.shape[0:2], figsize=(5, 1.25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "id": "G3ebdrvlyWvg",
    "outputId": "1c1e864c-6bd3-492b-8cca-5f5058342917",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# visualize the actual RGB channels.\n",
    "embeddings(_nala_params.reshape(-1, 3),\n",
    "           shape_=image.shape[0:2], figsize=(5, 1.25))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTptJUrR3ILQ",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **There is minimal resemblance to the true features**\n",
    "\n",
    "- This is unsurprising because there are no rules that define what the embedding should look like.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nyeX8d_L3RoO",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's try a bigger model\n",
    "\n",
    "### Builds the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HD9AHYqh8rpD",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "latent_dim = 12\n",
    "\n",
    "encoder = Encoder(latent_dim=latent_dim).to(device)\n",
    "decoder = Decoder(latent_dim=latent_dim).to(device)\n",
    "model = Autoencoder(encoder, decoder).to(device)\n",
    "\n",
    "# optimizer\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JnCrCI4_8rpD",
    "outputId": "0534483a-a3e3-449c-ab2b-b612d3816cd9",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# views the model\n",
    "\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f_DuA4Ta8rpE",
    "outputId": "ce01e347-efaa-4829-fbe6-2aca505e9f5b",
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# trains the model\n",
    "\n",
    "torch.manual_seed(0)\n",
    "Train(\n",
    "    model, encoder, decoder, dataloader, optimizer, 500,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U0rH1RJF8rpF",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Visualize the reconstruction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQiIfRNr8rpF",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# computes an example reconstruction for a minibatch\n",
    "\n",
    "(encoded_, decoded_) = transform_nn(next(iter(dataloader)), encoder, decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "Vw39umex8rpF",
    "outputId": "dd15aa3c-8691-4a82-c809-28c6b2cb76d6",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# plots a random example of the original and predicted spectra\n",
    "\n",
    "rand = np.random.randint(0, 512)\n",
    "plt.plot(spectras[rand, :, 0].cpu(), \"b\", label=\"Original\")\n",
    "plt.plot(decoded_[rand].squeeze(), \"r\", label=\"Generated\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9FvEzGbADuL",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Reconstruction is slightly better but just more overfit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Visualize the learned results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E6Y5kHZx8rpH",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# builds the spectra from the parameters\n",
    "\n",
    "(nala_spectra, _) = constructor.compute(_nala_params)\n",
    "\n",
    "# generated the encoded representation and decoded spectra\n",
    "\n",
    "(nala_encoded_, nala_decoded_) = transform_nn(nala_spectra, encoder, decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "Eb1YE0RE8rpH",
    "outputId": "6b31b706-2507-46b5-f64c-069c3ea7cad9",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# plots a random example of the original and predicted spectra\n",
    "\n",
    "rand = np.random.randint(0, nala_spectra.shape[0])\n",
    "plt.plot(nala_spectra[rand, :, 0].cpu(), \"b\", label=\"Original\")\n",
    "plt.plot(nala_decoded_[rand].squeeze(), \"r\", label=\"Generated\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "id": "s2BSXlJC8rpH",
    "outputId": "179e9e48-f3b9-4673-85f7-e9e94479d8ea",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the learned embeddings\n",
    "\n",
    "embeddings(nala_encoded_, shape_=image.shape[0:2], figsize=(5, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "id": "5FrPP3-Y8rpH",
    "outputId": "4a156602-00f5-4ee8-9f86-1356f43760e7",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# visualize the actual RGB channels.\n",
    "embeddings(_nala_params.reshape(-1, 3),\n",
    "           shape_=image.shape[0:2], figsize=(4, 1.25))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEqt9hGT8rpI",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Now there are just more features but no resemblance between the parameters.**\n",
    "- It would be impossible to have any resemblance to the features since it is overcomplete.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "glpKkqPCAjIR",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Physics constrained neural network\n",
    "\n",
    "### Building the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VObnDnsnVcs2",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class DensePhysLarger(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        x_vector,\n",
    "        model,\n",
    "        dense_params=3,\n",
    "        verbose=False,\n",
    "        device=\"cuda\",\n",
    "        num_channels=1,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            x_vector: The vector of values for x\n",
    "            model: the empirical function to fit\n",
    "            dense_params: number of output parameters to the model\n",
    "            verbose: sets if the model is verbose\n",
    "            device: device where the model will run\n",
    "            num_channels: number of channels in the input\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.dense_params = dense_params\n",
    "        self.x_vector = x_vector\n",
    "        self.verbose = verbose\n",
    "        self.num_channels = num_channels\n",
    "        self.device = device\n",
    "        self.model_params = kwargs.get(\"model_params\")\n",
    "        # (self.x_vector, size=(num_channels, dense_params // self.model_params))\n",
    "        self.model = model\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        n = 4\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.cuda()\n",
    "\n",
    "        # Input block of 1d convolution\n",
    "\n",
    "        self.hidden_x1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=self.num_channels,\n",
    "                      out_channels=8 * n, kernel_size=7),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=8 * n, out_channels=6 * n, kernel_size=7),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=6 * n, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "        )\n",
    "\n",
    "        self.hidden_x1_shape = self.hidden_x1(\n",
    "            torch.zeros(1, self.num_channels, self.x_vector.shape[0])\n",
    "        ).shape\n",
    "\n",
    "        # fully connected block\n",
    "\n",
    "        self.hidden_xfc = nn.Sequential(\n",
    "            nn.Linear(self.hidden_x1_shape[1] * self.hidden_x1_shape[2], 20),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(20, 20),\n",
    "            nn.SELU(),\n",
    "        )\n",
    "\n",
    "        # out of size 20\n",
    "\n",
    "        self.hidden_xfc_shape = self.hidden_xfc(\n",
    "            torch.zeros(1, self.hidden_x1_shape[1] * self.hidden_x1_shape[2])\n",
    "        ).shape\n",
    "\n",
    "        # 2nd block of 1d-conv layers\n",
    "\n",
    "        self.hidden_x2 = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Conv1d(in_channels=1, out_channels=4 * n, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4 * n, out_channels=4 * n, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4 * n, out_channels=4 * n, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4 * n, out_channels=4 * n, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4 * n, out_channels=4 * n, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4 * n, out_channels=4 * n, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.AvgPool1d(kernel_size=2),\n",
    "            nn.Conv1d(in_channels=4 * n, out_channels=2 * n, kernel_size=3),\n",
    "            nn.SELU(),\n",
    "            nn.AvgPool1d(kernel_size=2),\n",
    "            nn.Conv1d(in_channels=2 * n, out_channels=2, kernel_size=3),\n",
    "            nn.SELU(),\n",
    "            nn.AvgPool1d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.hidden_x2_shape = self.hidden_x2(\n",
    "            torch.zeros(\n",
    "                (\n",
    "                    self.hidden_xfc_shape[0],\n",
    "                    1,\n",
    "                    self.hidden_x1_shape[1] * self.hidden_x1_shape[2],\n",
    "                )\n",
    "            )\n",
    "        ).shape\n",
    "\n",
    "        # Flatten layer\n",
    "\n",
    "        self.flatten_layer = nn.Flatten()\n",
    "\n",
    "        # Final embedding block - Output 4 values - linear\n",
    "\n",
    "        self.hidden_embedding = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                self.hidden_x2_shape[1] * self.hidden_x2_shape[2]\n",
    "                + self.hidden_xfc_shape[1],\n",
    "                16,\n",
    "            ),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(8, self.dense_params),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, n=-1):\n",
    "\n",
    "        x = self.hidden_x1(x)\n",
    "        xfc = torch.reshape(x, (x.shape[0], -1))  # batch size, features\n",
    "        xfc = self.hidden_xfc(xfc)\n",
    "        x = torch.reshape(\n",
    "            x, (x.shape[0], 1, self.hidden_x1_shape[1]\n",
    "                * self.hidden_x1_shape[2])\n",
    "        )\n",
    "        x = self.hidden_x2(x)\n",
    "        cnn_flat = self.flatten_layer(x)\n",
    "        encoded = torch.cat((cnn_flat, xfc), 1)  # merge dense and 1d conv.\n",
    "\n",
    "        embedding = self.hidden_embedding(encoded)  # output is 3 parameters\n",
    "\n",
    "        embedding = torch.reshape(embedding, (embedding.shape[0], 3, -1))\n",
    "\n",
    "        embedding[:, 0, :] = (\n",
    "            embedding[:, 0, :] * self.model.amp_sd + self.model.amp_mean\n",
    "        )\n",
    "        embedding[:, 1, :] = (\n",
    "            embedding[:, 1, :] * self.model.phase_sd + self.model.phase_mean\n",
    "        )\n",
    "        embedding[:, 2, :] = (\n",
    "            embedding[:, 2, :] * self.model.frequency_sd +\n",
    "            self.model.frequency_mean\n",
    "        )\n",
    "\n",
    "        embedding = torch.reshape(embedding, (embedding.shape[0], -1))\n",
    "\n",
    "        embedding = torch.abs(embedding)\n",
    "        self.embed = embedding\n",
    "\n",
    "        (out, _) = self.model.compute(embedding, device=self.device)\n",
    "\n",
    "        out = torch.transpose(out, 1, 2)\n",
    "        out = torch.atleast_3d(out)\n",
    "\n",
    "        return (out.to(self.device), embedding.to(self.device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lKrvL1bxVcjn",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x_vector = torch.linspace(0, 10, 100)\n",
    "\n",
    "model = DensePhysLarger(\n",
    "    x_vector, constructor, dense_params=3, model_params=3, verbose=False\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O4QtxjDaXm0s",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# builds the dataloader\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    torch.transpose(spectra_full, 1, 2), batch_size=512, shuffle=True, num_workers=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nDHXFqcjfNy5",
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "random_seed(seed=42)\n",
    "\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)\n",
    "\n",
    "\n",
    "epochs = 200\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = 0.0\n",
    "    total_num = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for train_batch in dataloader:\n",
    "        pred, _ = model(train_batch.cuda())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = loss_func(train_batch.cuda(), pred)\n",
    "        loss.backward(create_graph=True)\n",
    "        train_loss += loss.item() * pred.shape[0]\n",
    "        total_num += pred.shape[0]\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= total_num\n",
    "\n",
    "    print(\"epoch : {}/{}, recon loss = {:.8f}\".format(epoch, epochs, train_loss))\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "bzpudYqD1uT6",
    "outputId": "55eef489-65e7-4bfb-bbe6-51194d44051e",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "spectra_generated, params = model(train_batch.cuda())\n",
    "rand = np.random.randint(0, 272)\n",
    "plt.plot(spectra_generated[rand, 0, :].detach().cpu().numpy(), \"r\")\n",
    "plt.plot(train_batch[rand, 0, :], \"b\")\n",
    "print(params[rand])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "o4YyAVHNSrQR",
    "outputId": "e1384f5f-1642-41e9-b990-8208dae26c5a",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nala_spectra_generated, nala_params = model(\n",
    "    nala_spectra.transpose(2, 1).cuda())\n",
    "rand = np.random.randint(0, nala_spectra_generated.shape[0])\n",
    "plt.plot(nala_spectra_generated[rand, 0, :].detach().cpu().numpy(), \"r\")\n",
    "plt.plot(nala_spectra[rand, :, 0], \"b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VtbZ7_hWqi-j",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# removes 2pi shifts\n",
    "nala_params[:, 1] = nala_params[:, 1] % 2 * np.pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "HBWPz48wpRKJ",
    "outputId": "1b681f09-4294-4350-c852-7ce3523dcdac",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the learned embeddings\n",
    "embeddings(nala_params.detach().cpu().numpy(),\n",
    "           shape_=image.shape[0:2], figsize=(5, 1.25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "id": "PlpTqUciqLcM",
    "outputId": "958b2239-a441-4bdf-c72e-9be17364b0ee",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# visualize the actual RGB channels.\n",
    "embeddings(_nala_params.reshape(-1, 3),\n",
    "           shape_=image.shape[0:2], figsize=(5, 1.25))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **results are much closer to the underlying physics since we enforced them**\n",
    "- The middle parameter is the phase. This is the hardest to learn $\\rightarrow$ this makes sense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqWzl7gZsYTE",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Try with a better optimizer AdaHessian\n",
    "\n",
    "- There are better optimizers than ADAM that use second-order information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JL5Xx1J-qXQW",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Sun Feb 26 16:34:00 2021\n",
    "@author: Amir Gholami\n",
    "@coauthor: David Samuel\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class AdaHessian(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Implements the AdaHessian algorithm from \"ADAHESSIAN: An Adaptive Second OrderOptimizer for Machine Learning\"\n",
    "    Arguments:\n",
    "        params (iterable) -- iterable of parameters to optimize or dicts defining parameter groups\n",
    "        lr (float, optional) -- learning rate (default: 0.1)\n",
    "        betas ((float, float), optional) -- coefficients used for computing running averages of gradient and the squared hessian trace (default: (0.9, 0.999))\n",
    "        eps (float, optional) -- term added to the denominator to improve numerical stability (default: 1e-8)\n",
    "        weight_decay (float, optional) -- weight decay (L2 penalty) (default: 0.0)\n",
    "        hessian_power (float, optional) -- exponent of the hessian trace (default: 1.0)\n",
    "        update_each (int, optional) -- compute the hessian trace approximation only after *this* number of steps (to save time) (default: 1)\n",
    "        n_samples (int, optional) -- how many times to sample `z` for the approximation of the hessian trace (default: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr=0.1,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-8,\n",
    "        weight_decay=0.0,\n",
    "        hessian_power=1.0,\n",
    "        update_each=1,\n",
    "        n_samples=1,\n",
    "        average_conv_kernel=False,\n",
    "    ):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(f\"Invalid epsilon value: {eps}\")\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n",
    "        if not 0.0 <= hessian_power <= 1.0:\n",
    "            raise ValueError(f\"Invalid Hessian power value: {hessian_power}\")\n",
    "\n",
    "        self.n_samples = n_samples\n",
    "        self.update_each = update_each\n",
    "        self.average_conv_kernel = average_conv_kernel\n",
    "\n",
    "        # use a separate generator that deterministically generates the same `z`s across all GPUs in case of distributed training\n",
    "        self.generator = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            betas=betas,\n",
    "            eps=eps,\n",
    "            weight_decay=weight_decay,\n",
    "            hessian_power=hessian_power,\n",
    "        )\n",
    "        super(AdaHessian, self).__init__(params, defaults)\n",
    "\n",
    "        for p in self.get_params():\n",
    "            p.hess = 0.0\n",
    "            self.state[p][\"hessian step\"] = 0\n",
    "\n",
    "    def get_params(self):\n",
    "        \"\"\"\n",
    "        Gets all parameters in all param_groups with gradients\n",
    "        \"\"\"\n",
    "\n",
    "        return (\n",
    "            p for group in self.param_groups for p in group[\"params\"] if p.requires_grad\n",
    "        )\n",
    "\n",
    "    def zero_hessian(self):\n",
    "        \"\"\"\n",
    "        Zeros out the accumulated hessian traces.\n",
    "        \"\"\"\n",
    "\n",
    "        for p in self.get_params():\n",
    "            if (\n",
    "                not isinstance(p.hess, float)\n",
    "                and self.state[p][\"hessian step\"] % self.update_each == 0\n",
    "            ):\n",
    "                p.hess.zero_()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def set_hessian(self):\n",
    "        \"\"\"\n",
    "        Computes the Hutchinson approximation of the hessian trace and accumulates it for each trainable parameter.\n",
    "        \"\"\"\n",
    "\n",
    "        params = []\n",
    "        for p in filter(lambda p: p.grad is not None, self.get_params()):\n",
    "            if (\n",
    "                self.state[p][\"hessian step\"] % self.update_each == 0\n",
    "            ):  # compute the trace only for each `update_each` step\n",
    "                params.append(p)\n",
    "            self.state[p][\"hessian step\"] += 1\n",
    "\n",
    "        if len(params) == 0:\n",
    "            return\n",
    "\n",
    "        if (\n",
    "            self.generator.device != params[0].device\n",
    "        ):  # hackish way of casting the generator to the right device\n",
    "            self.generator = torch.Generator(\n",
    "                params[0].device).manual_seed(2147483647)\n",
    "\n",
    "        grads = [p.grad for p in params]\n",
    "\n",
    "        for i in range(self.n_samples):\n",
    "            zs = [\n",
    "                torch.randint(0, 2, p.size(),\n",
    "                              generator=self.generator, device=p.device)\n",
    "                * 2.0\n",
    "                - 1.0\n",
    "                for p in params\n",
    "            ]  # Rademacher distribution {-1.0, 1.0}\n",
    "            h_zs = torch.autograd.grad(\n",
    "                grads,\n",
    "                params,\n",
    "                grad_outputs=zs,\n",
    "                only_inputs=True,\n",
    "                retain_graph=i < self.n_samples - 1,\n",
    "            )\n",
    "            for h_z, z, p in zip(h_zs, zs, params):\n",
    "                p.hess += (\n",
    "                    h_z * z / self.n_samples\n",
    "                )  # approximate the expected values of z*(H@z)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional) -- a closure that reevaluates the model and returns the loss (default: None)\n",
    "        \"\"\"\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        self.zero_hessian()\n",
    "        self.set_hessian()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None or p.hess is None:\n",
    "                    continue\n",
    "\n",
    "                if self.average_conv_kernel and p.dim() == 4:\n",
    "                    p.hess = (\n",
    "                        torch.abs(p.hess)\n",
    "                        .mean(dim=[2, 3], keepdim=True)\n",
    "                        .expand_as(p.hess)\n",
    "                        .clone()\n",
    "                    )\n",
    "\n",
    "                # Perform correct stepweight decay as in AdamW\n",
    "                p.mul_(1 - group[\"lr\"] * group[\"weight_decay\"])\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 1:\n",
    "                    state[\"step\"] = 0\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(\n",
    "                        p.data\n",
    "                    )  # Exponential moving average of gradient values\n",
    "                    state[\"exp_hessian_diag_sq\"] = torch.zeros_like(\n",
    "                        p.data\n",
    "                    )  # Exponential moving average of Hessian diagonal square values\n",
    "\n",
    "                exp_avg, exp_hessian_diag_sq = (\n",
    "                    state[\"exp_avg\"],\n",
    "                    state[\"exp_hessian_diag_sq\"],\n",
    "                )\n",
    "                beta1, beta2 = group[\"betas\"]\n",
    "                state[\"step\"] += 1\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(p.grad, alpha=1 - beta1)\n",
    "                exp_hessian_diag_sq.mul_(beta2).addcmul_(\n",
    "                    p.hess, p.hess, value=1 - beta2\n",
    "                )\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** state[\"step\"]\n",
    "                bias_correction2 = 1 - beta2 ** state[\"step\"]\n",
    "\n",
    "                k = group[\"hessian_power\"]\n",
    "                denom = (\n",
    "                    (exp_hessian_diag_sq / bias_correction2)\n",
    "                    .pow_(k / 2)\n",
    "                    .add_(group[\"eps\"])\n",
    "                )\n",
    "\n",
    "                # make update\n",
    "                step_size = group[\"lr\"] / bias_correction1\n",
    "                p.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Builds the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rxv9kJIOsyq0",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x_vector = torch.linspace(0, 10, 100)\n",
    "\n",
    "model = DensePhysLarger(\n",
    "    x_vector, constructor, dense_params=3, model_params=3, verbose=False\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qJwjRvbksyq0",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# builds the dataloader\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    torch.transpose(spectra_full, 1, 2), batch_size=512, shuffle=True, num_workers=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X-0ntsQZshWM",
    "outputId": "a0561ed6-4362-417b-d42a-5347dc4ae7ad",
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "random_seed(seed=42)\n",
    "\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "# Use AdaHessian\n",
    "\n",
    "optimizer = AdaHessian(model.parameters(), lr=0.1)\n",
    "\n",
    "epochs = 200\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = 0.0\n",
    "    total_num = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for train_batch in dataloader:\n",
    "        pred, _ = model(train_batch.cuda())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = loss_func(train_batch.cuda(), pred)\n",
    "        loss.backward(create_graph=True)\n",
    "        train_loss += loss.item() * pred.shape[0]\n",
    "        total_num += pred.shape[0]\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= total_num\n",
    "\n",
    "    print(\"epoch : {}/{}, recon loss = {:.8f}\".format(epoch, epochs, train_loss))\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yN_yCo6nshWM",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "spectra_generated, params = model(train_batch.cuda())\n",
    "rand = np.random.randint(0, 272)\n",
    "plt.plot(spectra_generated[rand, 0, :].detach().cpu().numpy(), \"r\")\n",
    "plt.plot(train_batch[rand, 0, :], \"b\")\n",
    "print(params[rand])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EpYzwfYDshWM",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nala_spectra_generated, nala_params = model(\n",
    "    nala_spectra.transpose(2, 1).cuda())\n",
    "rand = np.random.randint(0, nala_spectra_generated.shape[0])\n",
    "plt.plot(nala_spectra_generated[rand, 0, :].detach().cpu().numpy(), \"r\")\n",
    "plt.plot(nala_spectra[rand, :, 0], \"b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLc33BIqshWN",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nala_params[:, 1] = nala_params[:, 1] % 2 * np.pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W6_bQBfVshWN",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the learned embeddings\n",
    "embeddings(nala_params.detach().cpu().numpy(),\n",
    "           shape_=image.shape[0:2], figsize=(5, 1.25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YfpYeDHhshWN",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# visualize the actual RGB channels.\n",
    "embeddings(_nala_params.reshape(-1, 3),\n",
    "           shape_=image.shape[0:2], figsize=(5, 1.25))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is clearly the best result\n",
    "\n",
    "- It is quite impressive that we can build a feed forward model to fit data to complex functions\n",
    "- This is actually a very hard task for a neural network as frequency and phase are something that cannot be learned easily in convolutions\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Autoencoder Tutorial",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 ('m3_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d82ed07f1a000548bae641f7bea0b50abc9c5d353fc085ff35eceda152964b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}